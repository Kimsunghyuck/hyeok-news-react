name: Crawl News to Supabase

on:
  schedule:
    # í•˜ë£¨ 3ë²ˆ ì‹¤í–‰ (KST 09:00, 15:00, 19:00)
    - cron: '0 0 * * *'   # 00:00 UTC = 09:00 KST
    - cron: '0 6 * * *'   # 06:00 UTC = 15:00 KST
    - cron: '0 10 * * *'  # 10:00 UTC = 19:00 KST

  # ìˆ˜ë™ ì‹¤í–‰ ì˜µì…˜
  workflow_dispatch:
    inputs:
      test_mode:
        description: 'í…ŒìŠ¤íŠ¸ ëª¨ë“œ (ì†ŒëŸ‰ í¬ë¡¤ë§)'
        required: false
        default: 'false'

jobs:
  crawl:
    runs-on: ubuntu-latest

    steps:
      - name: ğŸ“¥ Checkout code
        uses: actions/checkout@v4

      - name: ğŸ Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: ğŸ“¦ Install dependencies
        run: |
          cd backend
          pip install -r requirements.txt

      - name: ğŸ” Run crawler and save to Supabase
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_KEY: ${{ secrets.SUPABASE_SERVICE_KEY }}
        run: |
          cd backend
          python crawler.py

      - name: ğŸ“Š Summary
        if: always()
        run: |
          echo "âœ… Crawling completed at $(date '+%Y-%m-%d %H:%M:%S KST')"
          echo "Check Supabase dashboard for results"

      - name: âŒ Notify on failure
        if: failure()
        run: |
          echo "::error::Crawling failed! Check logs for details."
