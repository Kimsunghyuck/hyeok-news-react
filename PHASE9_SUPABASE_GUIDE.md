# Phase 9: Supabase ë°ì´í„°ë² ì´ìŠ¤ í†µí•© - êµ¬í˜„ ê°€ì´ë“œ

> **ì‘ì„±ì¼**: 2025-12-22
> **ìƒíƒœ**: ì¤€ë¹„ ì™„ë£Œ - êµ¬í˜„ ëŒ€ê¸° ì¤‘
> **ì˜ˆìƒ ì‘ì—… ì‹œê°„**: 3-4ì‹œê°„

ì´ ë¬¸ì„œëŠ” Phase 9ì˜ ìƒì„¸ êµ¬í˜„ ê°€ì´ë“œì…ë‹ˆë‹¤. ë‹¨ê³„ë³„ë¡œ ë”°ë¼í•˜ë©´ Supabase DB í†µí•©ì„ ì™„ë£Œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

---

## ğŸ“‹ ì‚¬ì „ ì¤€ë¹„

### í•„ìš”í•œ ê³„ì •
- âœ… Supabase ê³„ì • (https://supabase.com - ë¬´ë£Œ)
- âœ… GitHub ê³„ì • (ì´ë¯¸ ìˆìŒ)

### í•„ìš”í•œ íŒŒì¼ (ê¸°ì¡´ í”„ë¡œì íŠ¸ì—ì„œ ë³µì‚¬)
```bash
# document-test í”„ë¡œì íŠ¸ì—ì„œ ë³µì‚¬í•  íŒŒì¼ë“¤
C:\VibeCoding\document-test\
â”œâ”€â”€ crawler.py
â”œâ”€â”€ parser.py
â”œâ”€â”€ config.py
â””â”€â”€ requirements.txt
```

---

## ğŸš€ Step 1: Supabase í”„ë¡œì íŠ¸ ìƒì„± (10ë¶„)

### 1.1 Supabase íšŒì›ê°€ì… ë° í”„ë¡œì íŠ¸ ìƒì„±

1. https://supabase.com ì ‘ì†
2. "Start your project" í´ë¦­
3. GitHub ê³„ì •ìœ¼ë¡œ ë¡œê·¸ì¸
4. "New Project" í´ë¦­

**í”„ë¡œì íŠ¸ ì„¤ì •**:
- Name: `hyeok-news-crawler`
- Database Password: **ê°•ë ¥í•œ ë¹„ë°€ë²ˆí˜¸ ì„¤ì •** (ì €ì¥ í•„ìˆ˜!)
- Region: `Northeast Asia (Seoul)` ë˜ëŠ” `Singapore`
- Pricing Plan: `Free` ì„ íƒ

5. "Create new project" í´ë¦­ (ì•½ 2ë¶„ ì†Œìš”)

### 1.2 API í‚¤ í™•ì¸

í”„ë¡œì íŠ¸ ìƒì„± í›„:
1. ì¢Œì¸¡ ë©”ë‰´ â†’ `Settings` â†’ `API`
2. ë‹¤ìŒ ê°’ë“¤ì„ ë³µì‚¬í•´ì„œ ì €ì¥:
   - **Project URL**: `https://xxxxx.supabase.co`
   - **anon public key**: `eyJhbGc...` (ê³µê°œ í‚¤ - React ì•±ìš©)
   - **service_role key**: `eyJhbGc...` (ë¹„ë°€ í‚¤ - í¬ë¡¤ëŸ¬ìš©, **ì ˆëŒ€ ê³µê°œ ê¸ˆì§€!**)

---

## ğŸ—„ï¸ Step 2: ë°ì´í„°ë² ì´ìŠ¤ í…Œì´ë¸” ìƒì„± (10ë¶„)

### 2.1 SQL Editor ì—´ê¸°

1. Supabase ëŒ€ì‹œë³´ë“œ ì¢Œì¸¡ ë©”ë‰´ â†’ `SQL Editor`
2. "New query" í´ë¦­

### 2.2 í…Œì´ë¸” ìƒì„± SQL ì‹¤í–‰

ì•„ë˜ SQLì„ ë³µì‚¬í•´ì„œ ë¶™ì—¬ë„£ê³  "Run" í´ë¦­:

```sql
-- ============================================
-- ë‰´ìŠ¤ í…Œì´ë¸” ìƒì„±
-- ============================================

CREATE TABLE IF NOT EXISTS news (
  -- ê¸°ë³¸ í‚¤
  id UUID DEFAULT gen_random_uuid() PRIMARY KEY,

  -- ë‰´ìŠ¤ ì •ë³´
  title TEXT NOT NULL,
  url TEXT UNIQUE NOT NULL,
  date DATE NOT NULL,

  -- ì¹´í…Œê³ ë¦¬ ë° ì¶œì²˜
  category TEXT NOT NULL,
  category_en TEXT,
  source TEXT NOT NULL,
  source_en TEXT,

  -- ì´ë¯¸ì§€
  image_url TEXT,

  -- ë©”íƒ€ë°ì´í„°
  scraped_at TIMESTAMPTZ DEFAULT NOW(),
  created_at TIMESTAMPTZ DEFAULT NOW(),
  updated_at TIMESTAMPTZ DEFAULT NOW()
);

-- ============================================
-- ì¸ë±ìŠ¤ ìƒì„± (ì„±ëŠ¥ ìµœì í™”)
-- ============================================

-- ì¹´í…Œê³ ë¦¬ë³„ ì¡°íšŒ ìµœì í™”
CREATE INDEX IF NOT EXISTS idx_news_category
ON news(category);

-- ì‹ ë¬¸ì‚¬ë³„ ì¡°íšŒ ìµœì í™”
CREATE INDEX IF NOT EXISTS idx_news_source
ON news(source);

-- ë‚ ì§œë³„ ì¡°íšŒ ìµœì í™” (ìµœì‹ ìˆœ ì •ë ¬)
CREATE INDEX IF NOT EXISTS idx_news_date
ON news(date DESC);

-- ì¹´í…Œê³ ë¦¬ + ë‚ ì§œ ë³µí•© ì¡°íšŒ ìµœì í™”
CREATE INDEX IF NOT EXISTS idx_news_category_date
ON news(category, date DESC);

-- ì‹ ë¬¸ì‚¬ + ë‚ ì§œ ë³µí•© ì¡°íšŒ ìµœì í™”
CREATE INDEX IF NOT EXISTS idx_news_source_date
ON news(source, date DESC);

-- ì¹´í…Œê³ ë¦¬ + ì‹ ë¬¸ì‚¬ + ë‚ ì§œ ë³µí•© ì¡°íšŒ ìµœì í™”
CREATE INDEX IF NOT EXISTS idx_news_category_source_date
ON news(category, source, date DESC);

-- URL ì¤‘ë³µ ë°©ì§€ (ì´ë¯¸ UNIQUE ì œì•½ì¡°ê±´ìœ¼ë¡œ ì²˜ë¦¬ë¨)

-- ============================================
-- updated_at ìë™ ì—…ë°ì´íŠ¸ íŠ¸ë¦¬ê±°
-- ============================================

CREATE OR REPLACE FUNCTION update_updated_at_column()
RETURNS TRIGGER AS $$
BEGIN
  NEW.updated_at = NOW();
  RETURN NEW;
END;
$$ LANGUAGE plpgsql;

CREATE TRIGGER update_news_updated_at
BEFORE UPDATE ON news
FOR EACH ROW
EXECUTE FUNCTION update_updated_at_column();

-- ============================================
-- Row Level Security (RLS) ì •ì±…
-- ============================================

-- RLS í™œì„±í™”
ALTER TABLE news ENABLE ROW LEVEL SECURITY;

-- ê³µê°œ ì½ê¸° ì •ì±… (ëˆ„êµ¬ë‚˜ ë‰´ìŠ¤ ì½ê¸° ê°€ëŠ¥)
CREATE POLICY "Enable read access for all users"
ON news FOR SELECT
USING (true);

-- ì„œë¹„ìŠ¤ ì—­í• ë§Œ ì‚½ì…/ì—…ë°ì´íŠ¸/ì‚­ì œ ê°€ëŠ¥
CREATE POLICY "Enable insert for service role only"
ON news FOR INSERT
WITH CHECK (auth.role() = 'service_role');

CREATE POLICY "Enable update for service role only"
ON news FOR UPDATE
USING (auth.role() = 'service_role');

CREATE POLICY "Enable delete for service role only"
ON news FOR DELETE
USING (auth.role() = 'service_role');
```

### 2.3 í…Œì´ë¸” í™•ì¸

1. ì¢Œì¸¡ ë©”ë‰´ â†’ `Table Editor`
2. `news` í…Œì´ë¸”ì´ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸
3. ì»¬ëŸ¼ ëª©ë¡ í™•ì¸:
   - id, title, url, date, category, source, image_url ë“±

---

## ğŸ Step 3: Python í¬ë¡¤ëŸ¬ íŒŒì¼ ì¤€ë¹„ (30ë¶„)

### 3.1 ê¸°ì¡´ í”„ë¡œì íŠ¸ì—ì„œ íŒŒì¼ ë³µì‚¬

```bash
# hyeok-news-react í”„ë¡œì íŠ¸ ë£¨íŠ¸ì— backend í´ë” ìƒì„±
mkdir backend
cd backend

# document-testì—ì„œ íŒŒì¼ ë³µì‚¬
copy C:\VibeCoding\document-test\crawler.py .
copy C:\VibeCoding\document-test\parser.py .
copy C:\VibeCoding\document-test\config.py .
copy C:\VibeCoding\document-test\requirements.txt .
```

### 3.2 requirements.txt ìˆ˜ì •

```txt
# ê¸°ì¡´ ì˜ì¡´ì„±
requests==2.31.0
beautifulsoup4==4.12.3
lxml==5.1.0
pytz==2024.1

# ì¶”ê°€: Supabase í´ë¼ì´ì–¸íŠ¸
supabase==2.3.4
```

### 3.3 ìƒˆ íŒŒì¼ ìƒì„±: `db_saver.py`

```python
"""
Supabase ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ëª¨ë“ˆ
"""

import os
from supabase import create_client, Client
from datetime import datetime
import pytz
from typing import List, Dict, Any

# Supabase ì„¤ì •
SUPABASE_URL = os.environ.get("SUPABASE_URL")
SUPABASE_SERVICE_KEY = os.environ.get("SUPABASE_SERVICE_KEY")

if not SUPABASE_URL or not SUPABASE_SERVICE_KEY:
    raise ValueError("âŒ SUPABASE_URL ë° SUPABASE_SERVICE_KEY í™˜ê²½ ë³€ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤!")

# Supabase í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
supabase: Client = create_client(SUPABASE_URL, SUPABASE_SERVICE_KEY)

# í•œêµ­ ì‹œê°„ëŒ€
KST = pytz.timezone('Asia/Seoul')


def save_news_to_db(news_items: List[Dict[str, Any]]) -> Dict[str, int]:
    """
    ë‰´ìŠ¤ ë°ì´í„°ë¥¼ Supabaseì— ì €ì¥

    Args:
        news_items: ë‰´ìŠ¤ ì•„ì´í…œ ë¦¬ìŠ¤íŠ¸

    Returns:
        {"success": ì„±ê³µ ê°œìˆ˜, "failed": ì‹¤íŒ¨ ê°œìˆ˜, "duplicate": ì¤‘ë³µ ê°œìˆ˜}
    """
    if not news_items:
        print("âš ï¸ ì €ì¥í•  ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return {"success": 0, "failed": 0, "duplicate": 0}

    success_count = 0
    failed_count = 0
    duplicate_count = 0

    print(f"\nğŸ“¦ ì´ {len(news_items)}ê°œ ë‰´ìŠ¤ë¥¼ DBì— ì €ì¥ ì¤‘...")

    for idx, item in enumerate(news_items, 1):
        try:
            # ë°ì´í„° ì¤€ë¹„
            data = {
                "title": item["title"],
                "url": item["url"],
                "date": item["date"],
                "category": item.get("category", item.get("main_category", "")),
                "category_en": item.get("category_en"),
                "source": item["source"],
                "source_en": item.get("source_en"),
                "image_url": item.get("image_url"),
                "scraped_at": item.get("scraped_at", datetime.now(KST).isoformat())
            }

            # Upsert: URLì´ ê°™ìœ¼ë©´ ì—…ë°ì´íŠ¸, ì—†ìœ¼ë©´ ì‚½ì…
            result = supabase.table("news").upsert(
                data,
                on_conflict="url"
            ).execute()

            # ì„±ê³µ
            if result.data:
                success_count += 1
                if idx % 10 == 0:  # 10ê°œë§ˆë‹¤ ì§„í–‰ ìƒí™© ì¶œë ¥
                    print(f"  ì§„í–‰: {idx}/{len(news_items)} ({success_count} ì„±ê³µ)")
            else:
                duplicate_count += 1

        except Exception as e:
            error_msg = str(e)

            # ì¤‘ë³µ í‚¤ ì—ëŸ¬ (ì´ë¯¸ ì¡´ì¬í•˜ëŠ” URL)
            if "duplicate key" in error_msg.lower() or "unique constraint" in error_msg.lower():
                duplicate_count += 1
            else:
                failed_count += 1
                print(f"  âŒ [{idx}] ì €ì¥ ì‹¤íŒ¨: {item['title'][:30]}...")
                print(f"     ì—ëŸ¬: {error_msg[:100]}")

    # ê²°ê³¼ ì¶œë ¥
    print(f"\nâœ… ì €ì¥ ì™„ë£Œ!")
    print(f"   ì„±ê³µ: {success_count}ê°œ")
    print(f"   ì¤‘ë³µ: {duplicate_count}ê°œ")
    print(f"   ì‹¤íŒ¨: {failed_count}ê°œ")

    return {
        "success": success_count,
        "failed": failed_count,
        "duplicate": duplicate_count
    }


def get_news_count_by_date(date: str) -> int:
    """
    íŠ¹ì • ë‚ ì§œì˜ ë‰´ìŠ¤ ê°œìˆ˜ ì¡°íšŒ

    Args:
        date: YYYY-MM-DD í˜•ì‹

    Returns:
        ë‰´ìŠ¤ ê°œìˆ˜
    """
    try:
        result = supabase.table("news") \
            .select("id", count="exact") \
            .eq("date", date) \
            .execute()

        return result.count if hasattr(result, 'count') else 0
    except Exception as e:
        print(f"âŒ ë‰´ìŠ¤ ê°œìˆ˜ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return 0


def delete_old_news(days: int = 30) -> int:
    """
    ì˜¤ë˜ëœ ë‰´ìŠ¤ ì‚­ì œ (ê¸°ë³¸ 30ì¼)

    Args:
        days: ì‚­ì œí•  ê¸°ì¤€ ì¼ìˆ˜

    Returns:
        ì‚­ì œëœ ë‰´ìŠ¤ ê°œìˆ˜
    """
    from datetime import timedelta

    cutoff_date = (datetime.now(KST) - timedelta(days=days)).date()

    try:
        result = supabase.table("news") \
            .delete() \
            .lt("date", str(cutoff_date)) \
            .execute()

        deleted_count = len(result.data) if result.data else 0
        print(f"ğŸ—‘ï¸ {cutoff_date} ì´ì „ ë‰´ìŠ¤ {deleted_count}ê°œ ì‚­ì œ ì™„ë£Œ")

        return deleted_count
    except Exception as e:
        print(f"âŒ ë‰´ìŠ¤ ì‚­ì œ ì‹¤íŒ¨: {e}")
        return 0


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    print("ğŸ§ª Supabase ì—°ê²° í…ŒìŠ¤íŠ¸...")

    # í…ŒìŠ¤íŠ¸ ë‰´ìŠ¤ ì €ì¥
    test_news = [{
        "title": "í…ŒìŠ¤íŠ¸ ë‰´ìŠ¤",
        "url": f"https://test.com/{datetime.now().timestamp()}",
        "date": datetime.now(KST).date().isoformat(),
        "category": "ì •ì¹˜",
        "source": "í…ŒìŠ¤íŠ¸",
        "image_url": "https://via.placeholder.com/300x200",
        "scraped_at": datetime.now(KST).isoformat()
    }]

    save_news_to_db(test_news)
    print("\nâœ… Supabase ì—°ê²° ì„±ê³µ!")
```

### 3.4 `crawler.py` ìˆ˜ì •

ê¸°ì¡´ `crawler.py`ì˜ `main()` í•¨ìˆ˜ë¥¼ ìˆ˜ì •:

```python
# crawler.py í•˜ë‹¨ì— ì¶”ê°€

from db_saver import save_news_to_db

def main():
    """ë©”ì¸ í¬ë¡¤ë§ í•¨ìˆ˜"""
    categories = ['politics', 'economy', 'society', 'international', 'culture', 'sports']
    sources = ['donga', 'chosun', 'joongang']

    all_news = []

    for category in categories:
        for source in sources:
            print(f"ğŸ” í¬ë¡¤ë§ ì¤‘: {category} - {source}")

            try:
                news_items = crawl_news(category, source)  # ê¸°ì¡´ í•¨ìˆ˜ ì‚¬ìš©
                all_news.extend(news_items)
                print(f"   âœ… {len(news_items)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
            except Exception as e:
                print(f"   âŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                continue

    # Supabaseì— ì €ì¥
    if all_news:
        save_news_to_db(all_news)
    else:
        print("âš ï¸ ìˆ˜ì§‘ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")

    print("\nâœ… ì „ì²´ í¬ë¡¤ë§ ì™„ë£Œ!")
```

---

## âš™ï¸ Step 4: GitHub Actions ì›Œí¬í”Œë¡œìš° ìƒì„± (20ë¶„)

### 4.1 ì›Œí¬í”Œë¡œìš° íŒŒì¼ ìƒì„±

```yaml
# .github/workflows/crawl-to-db.yml
name: Crawl News to Supabase

on:
  schedule:
    # í•˜ë£¨ 3ë²ˆ ì‹¤í–‰ (KST 09:00, 15:00, 19:00)
    - cron: '0 0 * * *'   # 00:00 UTC = 09:00 KST
    - cron: '0 6 * * *'   # 06:00 UTC = 15:00 KST
    - cron: '0 10 * * *'  # 10:00 UTC = 19:00 KST

  # ìˆ˜ë™ ì‹¤í–‰ ì˜µì…˜
  workflow_dispatch:
    inputs:
      test_mode:
        description: 'í…ŒìŠ¤íŠ¸ ëª¨ë“œ (ì†ŒëŸ‰ í¬ë¡¤ë§)'
        required: false
        default: 'false'

jobs:
  crawl:
    runs-on: ubuntu-latest

    steps:
      - name: ğŸ“¥ Checkout code
        uses: actions/checkout@v4

      - name: ğŸ Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: ğŸ“¦ Install dependencies
        run: |
          cd backend
          pip install -r requirements.txt

      - name: ğŸ” Run crawler and save to Supabase
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_KEY: ${{ secrets.SUPABASE_SERVICE_KEY }}
        run: |
          cd backend
          python crawler.py

      - name: ğŸ“Š Summary
        if: always()
        run: |
          echo "âœ… Crawling completed at $(date '+%Y-%m-%d %H:%M:%S KST')"
          echo "Check Supabase dashboard for results"

      - name: âŒ Notify on failure
        if: failure()
        run: |
          echo "::error::Crawling failed! Check logs for details."
```

### 4.2 GitHub Secrets ì„¤ì •

1. GitHub ì €ì¥ì†Œ í˜ì´ì§€ â†’ `Settings` â†’ `Secrets and variables` â†’ `Actions`
2. `New repository secret` í´ë¦­
3. ë‹¤ìŒ 2ê°œì˜ ì‹œí¬ë¦¿ ì¶”ê°€:

**Secret 1: SUPABASE_URL**
- Name: `SUPABASE_URL`
- Value: `https://xxxxx.supabase.co` (Step 1.2ì—ì„œ ë³µì‚¬í•œ Project URL)

**Secret 2: SUPABASE_SERVICE_KEY**
- Name: `SUPABASE_SERVICE_KEY`
- Value: `eyJhbGc...` (Step 1.2ì—ì„œ ë³µì‚¬í•œ service_role key)

---

## âš›ï¸ Step 5: React ì•± Supabase ì—°ë™ (1ì‹œê°„)

### 5.1 íŒ¨í‚¤ì§€ ì„¤ì¹˜

```bash
npm install @supabase/supabase-js
```

### 5.2 í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

`.env.local` íŒŒì¼ ìƒì„± (í”„ë¡œì íŠ¸ ë£¨íŠ¸):

```env
# Supabase
VITE_SUPABASE_URL=https://xxxxx.supabase.co
VITE_SUPABASE_ANON_KEY=eyJhbGc...  # anon public key

# Firebase (ê¸°ì¡´)
VITE_FIREBASE_API_KEY=...
# ...
```

**ì¤‘ìš”**: `.env.local`ì„ `.gitignore`ì— ì¶”ê°€ (ì´ë¯¸ ë˜ì–´ ìˆì–´ì•¼ í•¨)

### 5.3 Supabase í´ë¼ì´ì–¸íŠ¸ ìƒì„±

```typescript
// src/config/supabase.ts
import { createClient } from '@supabase/supabase-js'
import type { Database } from '../types/supabase.types'

const supabaseUrl = import.meta.env.VITE_SUPABASE_URL
const supabaseAnonKey = import.meta.env.VITE_SUPABASE_ANON_KEY

if (!supabaseUrl || !supabaseAnonKey) {
  throw new Error('Supabase í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!')
}

export const supabase = createClient<Database>(supabaseUrl, supabaseAnonKey)

console.log('âœ… Supabase í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ')
```

### 5.4 TypeScript íƒ€ì… ìƒì„± (ì˜µì…˜)

```typescript
// src/types/supabase.types.ts
export type Json =
  | string
  | number
  | boolean
  | null
  | { [key: string]: Json | undefined }
  | Json[]

export interface Database {
  public: {
    Tables: {
      news: {
        Row: {
          id: string
          title: string
          url: string
          date: string
          category: string
          category_en: string | null
          source: string
          source_en: string | null
          image_url: string | null
          scraped_at: string
          created_at: string
          updated_at: string
        }
        Insert: {
          id?: string
          title: string
          url: string
          date: string
          category: string
          category_en?: string | null
          source: string
          source_en?: string | null
          image_url?: string | null
          scraped_at?: string
          created_at?: string
          updated_at?: string
        }
        Update: {
          id?: string
          title?: string
          url?: string
          date?: string
          category?: string
          category_en?: string | null
          source?: string
          source_en?: string | null
          image_url?: string | null
          scraped_at?: string
          created_at?: string
          updated_at?: string
        }
      }
    }
  }
}
```

### 5.5 useNews Hook ìˆ˜ì •

```typescript
// src/hooks/useNews.ts
import { useState, useEffect } from 'react'
import { supabase } from '../config/supabase'
import type { NewsItem, CategoryId, SourceId } from '../types/news.types'

interface UseNewsReturn {
  news: NewsItem[]
  loading: boolean
  error: Error | null
  refetch: () => Promise<void>
}

export function useNews(
  category: CategoryId,
  source: SourceId,
  date: string
): UseNewsReturn {
  const [news, setNews] = useState<NewsItem[]>([])
  const [loading, setLoading] = useState(true)
  const [error, setError] = useState<Error | null>(null)

  const loadNews = async () => {
    setLoading(true)
    setError(null)

    try {
      // Supabaseì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
      const { data, error: supabaseError } = await supabase
        .from('news')
        .select('*')
        .eq('category', category)
        .eq('source', source)
        .eq('date', date)
        .order('scraped_at', { ascending: false })

      if (supabaseError) {
        throw supabaseError
      }

      // NewsItem íƒ€ì…ìœ¼ë¡œ ë³€í™˜
      const newsItems: NewsItem[] = (data || []).map(item => ({
        title: item.title,
        url: item.url,
        date: item.date,
        category: item.category,
        category_en: item.category_en || undefined,
        source: item.source,
        source_en: item.source_en || undefined,
        image_url: item.image_url || undefined,
        scraped_at: item.scraped_at
      }))

      setNews(newsItems)
      console.log(`âœ… ë‰´ìŠ¤ ë¡œë“œ ì„±ê³µ: ${newsItems.length}ê°œ`)
    } catch (err) {
      setError(err as Error)
      console.error('âŒ ë‰´ìŠ¤ ë¡œë“œ ì‹¤íŒ¨:', err)
    } finally {
      setLoading(false)
    }
  }

  useEffect(() => {
    loadNews()
  }, [category, source, date])

  return { news, loading, error, refetch: loadNews }
}
```

### 5.6 HomeDashboard ìˆ˜ì • (ì˜µì…˜ - ìµœì‹  ë‰´ìŠ¤ë§Œ ë¡œë“œ)

```typescript
// src/hooks/useLatestNews.ts
import { useState, useEffect } from 'react'
import { supabase } from '../config/supabase'
import type { NewsItem, CategoryId } from '../types/news.types'

export function useLatestNews(category: CategoryId, limit: number = 3) {
  const [news, setNews] = useState<NewsItem[]>([])
  const [loading, setLoading] = useState(true)

  useEffect(() => {
    const loadLatestNews = async () => {
      setLoading(true)

      try {
        const { data } = await supabase
          .from('news')
          .select('*')
          .eq('category', category)
          .order('scraped_at', { ascending: false })
          .limit(limit)

        setNews(data as NewsItem[] || [])
      } catch (err) {
        console.error('ìµœì‹  ë‰´ìŠ¤ ë¡œë“œ ì‹¤íŒ¨:', err)
      } finally {
        setLoading(false)
      }
    }

    loadLatestNews()
  }, [category, limit])

  return { news, loading }
}
```

---

## âœ… Step 6: í…ŒìŠ¤íŠ¸ ë° ê²€ì¦ (30ë¶„)

### 6.1 ë¡œì»¬ Python í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸

```bash
cd backend

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (Windows PowerShell)
$env:SUPABASE_URL="https://xxxxx.supabase.co"
$env:SUPABASE_SERVICE_KEY="eyJhbGc..."

# í¬ë¡¤ëŸ¬ ì‹¤í–‰
python crawler.py
```

**ì˜ˆìƒ ì¶œë ¥**:
```
ğŸ” í¬ë¡¤ë§ ì¤‘: politics - donga
   âœ… 10ê°œ ìˆ˜ì§‘ ì™„ë£Œ
ğŸ” í¬ë¡¤ë§ ì¤‘: politics - chosun
   âœ… 8ê°œ ìˆ˜ì§‘ ì™„ë£Œ
...
ğŸ“¦ ì´ 150ê°œ ë‰´ìŠ¤ë¥¼ DBì— ì €ì¥ ì¤‘...
  ì§„í–‰: 10/150 (10 ì„±ê³µ)
  ì§„í–‰: 20/150 (20 ì„±ê³µ)
...
âœ… ì €ì¥ ì™„ë£Œ!
   ì„±ê³µ: 150ê°œ
   ì¤‘ë³µ: 0ê°œ
   ì‹¤íŒ¨: 0ê°œ
```

### 6.2 Supabase ëŒ€ì‹œë³´ë“œì—ì„œ í™•ì¸

1. Supabase ëŒ€ì‹œë³´ë“œ â†’ `Table Editor` â†’ `news` í…Œì´ë¸”
2. ì €ì¥ëœ ë‰´ìŠ¤ ë°ì´í„° í™•ì¸

### 6.3 React ì•± í…ŒìŠ¤íŠ¸

```bash
npm run dev
```

1. ë¸Œë¼ìš°ì € ì½˜ì†”ì—ì„œ "âœ… ë‰´ìŠ¤ ë¡œë“œ ì„±ê³µ: Xê°œ" ë©”ì‹œì§€ í™•ì¸
2. ë‰´ìŠ¤ ê·¸ë¦¬ë“œì— ë°ì´í„° í‘œì‹œ í™•ì¸
3. ì¹´í…Œê³ ë¦¬/ì‹ ë¬¸ì‚¬ í•„í„° ë™ì‘ í™•ì¸

### 6.4 GitHub Actions ìˆ˜ë™ ì‹¤í–‰ í…ŒìŠ¤íŠ¸

1. GitHub ì €ì¥ì†Œ â†’ `Actions` íƒ­
2. `Crawl News to Supabase` ì›Œí¬í”Œë¡œìš° ì„ íƒ
3. `Run workflow` ë²„íŠ¼ í´ë¦­
4. ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ë¡œê·¸ í™•ì¸

---

## ğŸ“Š ì™„ë£Œ ì²´í¬ë¦¬ìŠ¤íŠ¸

### Phase 9.1: Supabase í”„ë¡œì íŠ¸ ìƒì„±
- [ ] Supabase ê³„ì • ìƒì„±
- [ ] í”„ë¡œì íŠ¸ ìƒì„± ì™„ë£Œ
- [ ] API í‚¤ ì €ì¥ (URL, anon key, service key)

### Phase 9.2: ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •
- [ ] `news` í…Œì´ë¸” ìƒì„±
- [ ] ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ
- [ ] RLS ì •ì±… ì„¤ì • ì™„ë£Œ

### Phase 9.3: Python í¬ë¡¤ëŸ¬ êµ¬í˜„
- [ ] `backend/` í´ë” ìƒì„±
- [ ] ê¸°ì¡´ íŒŒì¼ ë³µì‚¬ ì™„ë£Œ
- [ ] `db_saver.py` ìƒì„±
- [ ] `crawler.py` ìˆ˜ì •
- [ ] ë¡œì»¬ í…ŒìŠ¤íŠ¸ ì„±ê³µ

### Phase 9.4: GitHub Actions ì„¤ì •
- [ ] `.github/workflows/crawl-to-db.yml` ìƒì„±
- [ ] GitHub Secrets ì„¤ì • ì™„ë£Œ
- [ ] ìˆ˜ë™ ì‹¤í–‰ í…ŒìŠ¤íŠ¸ ì„±ê³µ

### Phase 9.5: React ì•± ì—°ë™
- [ ] `@supabase/supabase-js` ì„¤ì¹˜
- [ ] `.env.local` ì„¤ì •
- [ ] `src/config/supabase.ts` ìƒì„±
- [ ] `useNews.ts` ìˆ˜ì •
- [ ] ë¡œì»¬ í…ŒìŠ¤íŠ¸ ì„±ê³µ

---

## ğŸ‰ ì™„ë£Œ í›„ ë‹¤ìŒ ë‹¨ê³„

Phase 9 ì™„ë£Œ í›„:
1. âœ… JSON íŒŒì¼ ì œê±° (ë” ì´ìƒ ë¶ˆí•„ìš”)
2. âœ… `public/data/` í´ë” ì •ë¦¬
3. âœ… README.md ì—…ë°ì´íŠ¸
4. âœ… PROGRESS.md ì—…ë°ì´íŠ¸ (Phase 9 ì™„ë£Œ ì²´í¬)

---

## ğŸ†˜ ë¬¸ì œ í•´ê²°

### Q1: "Supabase ì—°ê²° ì‹¤íŒ¨" ì—ëŸ¬
**í•´ê²°**:
- í™˜ê²½ ë³€ìˆ˜ í™•ì¸ (`SUPABASE_URL`, `SUPABASE_SERVICE_KEY`)
- API í‚¤ê°€ ì˜¬ë°”ë¥¸ì§€ í™•ì¸
- ë„¤íŠ¸ì›Œí¬ ì—°ê²° í™•ì¸

### Q2: "RLS ì •ì±…ìœ¼ë¡œ ì¸í•œ ì ‘ê·¼ ê±°ë¶€" ì—ëŸ¬
**í•´ê²°**:
- Python í¬ë¡¤ëŸ¬: `service_role` key ì‚¬ìš© í™•ì¸
- React ì•±: `anon` key ì‚¬ìš© í™•ì¸
- RLS ì •ì±… ì¬í™•ì¸

### Q3: "ì¤‘ë³µ í‚¤" ì—ëŸ¬
**í•´ê²°**:
- ì •ìƒ ë™ì‘ (ì´ë¯¸ ì¡´ì¬í•˜ëŠ” URLì€ ì—…ë°ì´íŠ¸ë¨)
- `upsert` ëŒ€ì‹  `insert`ë¥¼ ì‚¬ìš©í–ˆë‹¤ë©´ `upsert`ë¡œ ë³€ê²½

---

**ì´ ë¬¸ì„œë¥¼ ë”°ë¼í•˜ë©´ Phase 9ë¥¼ ì™„ë£Œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!** ğŸš€
